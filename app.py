#streamlit run app.py -- --model_path weights.pt
import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import cv2

from PIL import Image, ImageOps
import torchvision.transforms as T
import torchvision.models as models
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

from tqdm import tqdm
import os

import argparse
import torch

from models import *
from yolox import *
from det_processing_func import *

st.set_page_config(page_title="Building AI Scan", layout="wide", page_icon="üõ∞Ô∏è")



# --- üé® CSS –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø ---
st.markdown("""
    <style>
    /* 1. –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–Ω –∏ —à—Ä–∏—Ñ—Ç */
    .stApp {
        background-color: #F5F7F9; /* –û—á–µ–Ω—å —Å–≤–µ—Ç–ª–æ-—Å–µ—Ä—ã–π, –ø—Ä–∏—è—Ç–Ω—ã–π –¥–ª—è –≥–ª–∞–∑ */
    }
    
    /* 2. –ó–∞–≥–æ–ª–æ–≤–∫–∏ */

    h2, h3 {
        color: #37474F;
    }

    /* 3. –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è Sidebar */
    [data-testid="stSidebar"] {
        background-color: #FFFFFF;
        border-right: 1px solid #E0E0E0;
    }

    /* 4. –°—Ç–∏–ª—å –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ –º–µ—Ç—Ä–∏–∫ (GSD, –ü–ª–æ—â–∞–¥—å) */
    [data-testid="stMetric"] {
        background-color: #FFFFFF;
        border: 1px solid #E0E0E0;
        padding: 15px;
        border-radius: 5px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05); /* –õ–µ–≥–∫–∞—è —Ç–µ–Ω—å */
        text-align: center;
    }
    [data-testid="stMetric"]:hover {
        transform: scale(1.02); /* –≠—Ñ—Ñ–µ–∫—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏ */
    }
    
    /* –¶–≤–µ—Ç —Ü–∏—Ñ—Ä –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö */
    [data-testid="stMetricValue"] {
        font-size: 28px;
        color: #0c343d; 
        font-weight: bold;
    }

    /* 5. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Å–∫—Ä—É–≥–ª–µ–Ω–∏–µ —É–≥–ª–æ–≤) */
    img {
        border-radius: 8px;
        border: 1px solid #ddd;
    }
    
    /* 6. –ö–Ω–æ–ø–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (File Uploader) */
    [data-testid="stFileUploader"] {
        border: 2px dashed #2E7D32;
        border-radius: 10px;
        padding: 20px;
        background-color: #FAFAFA;
    }
    </style>
    """, unsafe_allow_html=True)

SEG_MODEL_PATH = 'weights/aspp_unet_resnet34_best_val_iou.pt'
DET_MODEL_PATH = 'weights/yolox_L_best_mAP.pt'

@st.cache_resource
def load_my_model(path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = UNetASPPResNet34().to(device)
    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint['state_model'])
    model.eval()
    return model, device

@st.cache_resource
def load_detector(path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = YOLOX().to(device)
    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint['state_model'])
    model.eval()
    return model


# --- 2. –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò (–Ω–∞—à–∞ analyze_single_picture) ---
def process_image(img_pil, model, device):
    w_orig, h_orig = img_pil.size
    max_side = max(w_orig, h_orig)
    
    # Padding –¥–æ –∫–≤–∞–¥—Ä–∞—Ç–∞
    padding = (0, 0, max_side - w_orig, max_side - h_orig)
    img_padded = ImageOps.expand(img_pil, padding, fill=0)
    
    transform = T.Compose([
        T.Resize((512, 512)),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    input_tensor = transform(img_padded).unsqueeze(0).to(device)

    with torch.no_grad():
        logits, pred_gsd = model(input_tensor)
        proba_map = torch.sigmoid(logits).cpu().squeeze().numpy()
        gsd_val = pred_gsd.item()

    # –û–±—Ä–∞—Ç–Ω—ã–π —Ä–µ—Å–∞–π–∑ –∏ –æ–±—Ä–µ–∑–∫–∞
    full_mask = cv2.resize(proba_map, (max_side, max_side), interpolation=cv2.INTER_NEAREST)
    final_mask = full_mask[0:h_orig, 0:w_orig]
    binary_mask = (final_mask > 0.5).astype(np.uint8)
    
    # –ü–ª–æ—â–∞–¥—å
    total_area = np.sum(binary_mask) * (gsd_val ** 2)
    
    return binary_mask, total_area, gsd_val

# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---
st.set_page_config(page_title="Building AI Scan", layout="wide")
st.title("üõ∞Ô∏è –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—â–∞–¥–∏ –∑–∞—Å—Ç—Ä–æ–π–∫–∏")
st.markdown("**–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–ª–æ—â–∞–¥–∏ –∑–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–º—É –∏–ª–∏ –∞—ç—Ä–æ—Ñ–æ—Ç–æ—Å–Ω–∏–º–∫—É**")

uploaded_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–π —Å–Ω–∏–º–æ–∫", type=["jpg", "jpeg", "png", "tif"])
model_path = SEG_MODEL_PATH

if uploaded_file:
    model, device = load_my_model(model_path)
    detector = load_detector(DET_MODEL_PATH)

    image = Image.open(uploaded_file).convert("RGB")    
    
    col1, col2, col3 = st.columns([1, 1, 1])
    
    with col1:
        st.image(image, caption="–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", width='stretch')
    
    with st.spinner("–ù–µ–π—Ä–æ—Å–µ—Ç—å —Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä—ã..."):
        mask, area, gsd = process_image(image, model, device)

        
    with col2:
        # –ö—Ä–∞—Å–∏–≤–æ–µ –Ω–∞–ª–æ–∂–µ–Ω–∏–µ –º–∞—Å–∫–∏
        img_np = np.array(image)
        overlay = img_np.copy()
        overlay[mask > 0] = [255, 0, 255]
        blended = cv2.addWeighted(img_np, 0.7, overlay, 0.3, 0)
        st.image(blended, caption="–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", width='stretch')

    with col3:
        mask_to_show = (mask * 255).astype(np.uint8)
        img_mask = Image.fromarray(mask_to_show)
        st.image(img_mask, caption="–ú–∞—Å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", width='stretch')

    # –ú–µ—Ç—Ä–∏–∫–∏
    m_col1, m_col2 = st.columns(2)
    m_col1.metric("–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å", f"{area:.1f} –º¬≤")
    m_col2.metric("GSD (–º–∞—Å—à—Ç–∞–±)", f"{gsd:.2f} –º/–ø–∫—Å")

    st.divider()

# - - - –î–ï–¢–ï–ö–¶–ò–Ø
    st.header("üîçÔ∏è –ü–æ–∏—Å–∫ –∏ –ø–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–µ–Ω–∏–π")
    final_global_bboxes = visualize_detection_inference_streamlit(model = detector,
        pil_image_input = image,
        device = device,
        conf_threshold=0.2,
        iou_threshold=0.45,
        patch_size=1024,
        overlap_ratio=0.2,
        image_name="Uploaded Image")
